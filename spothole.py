# -*- coding: utf-8 -*-
"""SpotHole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kj8dSXTB9oDM98fsBzQi7jqwZ7Dju2wo
"""

def isPotHole(test_data):
  from sklearn.preprocessing import StandardScaler
  from sklearn.externals.joblib import dump, load

  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import scipy.fftpack
  from sklearn import preprocessing
  from sklearn.metrics import confusion_matrix, f1_score
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import accuracy_score
  from sklearn.svm import SVC
  from sklearn.decomposition import PCA
  from sklearn.preprocessing import StandardScaler
  import keras
  from keras.models import Sequential
  from keras.layers import Dense, Dropout
  import itertools


  model_nn = Sequential()
  model_nn.add(Dense(units=100,activation="relu",input_dim = 72))
  model_nn.add(Dropout(0.3))
  model_nn.add(Dense(units=50,activation="relu"))
  model_nn.add(Dropout(0.3))
  model_nn.add(Dense(units=10,activation="relu"))
  model_nn.add(Dropout(0.3))
  model_nn.add(Dense(units=2,activation="softmax"))       # 3 units for 3 classes
  model_nn.compile(optimizer='adam',loss="categorical_crossentropy",metrics=["accuracy"])
  model_nn.load_weights('weights.h5')


  data = pd.read_json(test_data)
  df_main_test = pd.read_csv('./features_test.txt')
  
  sc=load('std_scaler.bin')


  # Feature Engineering
  dt = data
  

  a = dt.mean()      # will give an array of mean of columns of dt
  
  # print(b, dty)
  mean_ax = a[1]
  mean_ay = a[2]
  mean_az = a[3]

  mean_gx = a[4]
  mean_gy = a[5]
  mean_gz = a[6]

  # min
  a = dt.min()
  min_ax = a[1]
  min_ay = a[2]
  min_az = a[3]

  min_gx = a[4]
  min_gy = a[5]
  min_gz = a[6]

  # max
  a = dt.max()
  max_ax = a[1]
  max_ay = a[2]
  max_az = a[3]

  max_gx = a[4]
  max_gy = a[5]
  max_gz = a[6]

  # std dev
  a = dt.std()
  sd_ax = a[1]
  sd_ay = a[2]
  sd_az = a[3]

  sd_gx = a[4]
  sd_gy = a[5]
  sd_gz = a[6]

  # variance
  a = dt.var()
  var_ax = a[1]
  var_ay = a[2]
  var_az = a[3]

  var_gx = a[4]
  var_gy = a[5]
  var_gz = a[6]

  #adding max-min
  mm_x = max_ax - min_ax
  mm_y = max_ay - min_ay
  mm_z = max_az - min_az


  # median coln wise of acc data
  a = dt.median()
  med_ax = a[1]
  med_ay = a[2]
  med_az = a[3]

  med_gx = a[4]
  med_gy = a[5]
  med_gz = a[6]

  # entropy coln wise of acc data

  # interquantile ranges
  a = dt.quantile(.25)
  quant1_ax = a[1]
  quant1_ay = a[2]
  quant1_az = a[3]

  quant1_gx = a[4]
  quant1_gy = a[5]
  quant1_gz = a[6]

  a = dt.quantile(.5)
  quant2_ax = a[1]
  quant2_ay = a[2]
  quant2_az = a[3]

  quant2_gx = a[4]
  quant2_gy = a[5]
  quant2_gz = a[6]

  a = dt.quantile(.75)
  quant3_ax = a[1]
  quant3_ay = a[2]
  quant3_az = a[3]

  quant3_gx = a[4]
  quant3_gy = a[5]
  quant3_gz = a[6]


  # mean absolute deviation
  a = dt.mad()
  mad_ax = a[1]
  mad_ay = a[2]
  mad_az = a[3]

  mad_gx = a[4]
  mad_gy = a[5]
  mad_gz = a[6]

  # skewness 
  a = dt.skew()
  skew_ax = a[1]
  skew_ay = a[2]
  skew_az = a[3]

  skew_gx = a[4]
  skew_gy = a[5]
  skew_gz = a[6]


  # gradient based features : gradient with respect to timestamp

  #taking gradients
  # arx = dt['accx']
  # ary = dt['accy']
  # arz = dt['accz']

  # grx = dt['gyrx']
  # gry = dt['gyry']
  # grz = dt['gyrz']

  # tm = dt['timestamp']
  # # adx = np.gradient(arx, tm).max()
  # ady = np.gradient(ary, tm).max()
  # adz = np.gradient(arz, tm).max()
  # gdx = np.gradient(grx, tm).max()
  # gdy = np.gradient(gry, tm).max()
  # gdz = np.gradient(grz, tm).max()


  # frequency domain features : fft , spectral energy ,   

  #taking fourier transforms
  ft = scipy.fftpack.fft(dt)

  fft_ax = ft[1].max().imag
  fft_ay = ft[2].max().imag
  fft_az = ft[3].max().imag

  #getting spectral energy
  sp_ax = np.mean(np.square(ft[1].real) + np.square(ft[1].imag))
  sp_ay = np.mean(np.square(ft[2].real) + np.square(ft[2].imag))
  sp_az = np.mean(np.square(ft[3].real) + np.square(ft[3].imag))



  df_main_test = pd.DataFrame([[mean_ax,mean_ay,mean_az,mean_gx,mean_gy,mean_gz,sd_ax,
                            sd_ay,sd_az,sd_gx,sd_gy,sd_gz,min_ax,min_ay,min_az,min_gx,min_gy,min_gz,
                            max_ax,max_ay,max_az,max_gx,max_gy,max_gz,var_ax,var_ay,var_az,var_gx,var_gy,
                            var_gz,med_ax,med_ay,med_az,med_gx,med_gy,med_gz,quant1_ax,quant1_ay,quant1_az
                            ,quant1_gx,quant1_gy,quant1_gz,quant2_ax,quant2_ay,quant2_az,quant2_gx,
                            quant2_gy,quant2_gz,quant3_ax,quant3_ay,quant3_az,quant3_gx,quant3_gy,
                            quant3_gz,mad_ax,mad_ay,mad_az,mad_gx,mad_gy,mad_gz,skew_ax,skew_ay,
                            skew_az,skew_gx,skew_gy,skew_gz,fft_ax,fft_ay,fft_az,
                            sp_ax,sp_ay,sp_az]], 

                        columns = ('mean_ax','mean_ay','mean_az','mean_gx','mean_gy',
                                    'mean_gz','sd_ax','sd_ay','sd_az','sd_gx','sd_gy','sd_gz','min_ax','min_ay'
                                    ,'min_az',
                                    'min_gx','min_gy','min_gz','max_ax','max_ay','max_az','max_gx','max_gy','max_gz',
                                    'var_ax','var_ay','var_az','var_gx','var_gy','var_gz','med_ax','med_ay'
                                    ,'med_az','med_gx',
                                    'med_gy','med_gz','quant1_ax','quant1_ay','quant1_az','quant1_gx',
                                    'quant1_gy',
                                    'quant1_gz','quant2_ax','quant2_ay','quant2_az','quant2_gx','quant2_gy'
                                    ,
                                    'quant2_gz','quant3_ax','quant3_ay','quant3_az','quant3_gx','quant3_gy',
                                    'quant3_gz',
                                    'mad_ax','mad_ay','mad_az','mad_gx','mad_gy','mad_gz','skew_ax',
                                    'skew_ay','skew_az',
                                    'skew_gx','skew_gy','skew_gz'
                                    ,'fft_ax','fft_ay','fft_az',
                                    'sp_ax','sp_ay','sp_az'))

  df_main_test['fft_ax'] = preprocessing.scale(df_main_test['fft_ax'])
  df_main_test['fft_ay'] = preprocessing.scale(df_main_test['fft_ay'])
  df_main_test['fft_az'] = preprocessing.scale(df_main_test['fft_az'])

  df_main_test['sp_ax'] = preprocessing.scale(df_main_test['sp_ax'])
  df_main_test['sp_ay'] = preprocessing.scale(df_main_test['sp_ay'])
  df_main_test['sp_az'] = preprocessing.scale(df_main_test['sp_az'])
  
  standardized_data = sc.transform(df_main_test)

  x_test = standardized_data

  pred = model_nn.predict(x_test)

  return pred

print(isPotHole(test_data))

import json

test_data = '[ {"timestamp":1000, "accX": ".045", "accY": "1.69", "accZ": "11.69", "gyroX":"-0.0004", "gyroY":"-0.002", "gyroZ":"-0.0015", "lat":"25.42", "lon":"81.77"}, {"timestamp":1000, "accX": ".061", "accY": "1.51", "accZ": "11.67", "gyroX":"-0.0404", "gyroY":"-0.602", "gyroZ":"-0.9015", "lat":"75.62", "lon":"89.7"}, {"timestamp":1000, "accX": ".045", "accY": "1.69", "accZ": "11.69", "gyroX":"-0.0004", "gyroY":"-0.002", "gyroZ":"-0.0015", "lat":"25.42", "lon":"81.77"}, {"timestamp":1000, "accX": ".045", "accY": "1.69", "accZ": "11.69", "gyroX":"-0.0004", "gyroY":"-0.002", "gyroZ":"-0.0015", "lat":"25.42", "lon":"81.77"} ]'

# test_data = json.loads(test_data)

